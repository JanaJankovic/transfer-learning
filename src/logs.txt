2024-09-28 23:38:12.554571: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
WARNING:tensorflow:From C:\Users\user\AppData\Local\pypoetry\Cache\virtualenvs\transfer-learning-c0qmjA8b-py3.9\lib\site-packages\keras\src\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.

2024-09-28 23:38:35.881696: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: SSE SSE2 SSE3 SSE4.1 SSE4.2 AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
WARNING:tensorflow:From C:\Users\user\AppData\Local\pypoetry\Cache\virtualenvs\transfer-learning-c0qmjA8b-py3.9\lib\site-packages\keras\src\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.

[I 2024-09-28 23:38:36,941] A new study created in memory with name: no-name-7eb96c9c-681f-4f04-9934-156faa4eb453
WARNING:tensorflow:From C:\Users\user\AppData\Local\pypoetry\Cache\virtualenvs\transfer-learning-c0qmjA8b-py3.9\lib\site-packages\keras\src\utils\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.

WARNING:tensorflow:From C:\Users\user\AppData\Local\pypoetry\Cache\virtualenvs\transfer-learning-c0qmjA8b-py3.9\lib\site-packages\keras\src\engine\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.

[I 2024-09-28 23:38:47,980] Trial 0 finished with value: 0.28708582880622463 and parameters: {'learning_rate': 0.0001, 'batch_size': 16, 'network_type': 'GRU', 'num_layers': 1, 'num_neurons': 64, 'dropout_rate': 0.3}. Best is trial 0 with value: 0.28708582880622463.
[I 2024-09-28 23:38:52,140] Trial 1 finished with value: 0.27329514920084097 and parameters: {'learning_rate': 0.01, 'batch_size': 32, 'network_type': 'RNN', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.3}. Best is trial 1 with value: 0.27329514920084097.
[I 2024-09-28 23:38:58,752] Trial 2 finished with value: 0.1395852454336066 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'GRU', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:39:04,110] Trial 3 finished with value: 0.4144358642728705 and parameters: {'learning_rate': 0.01, 'batch_size': 32, 'network_type': 'RNN', 'num_layers': 2, 'num_neurons': 128, 'dropout_rate': 0.3}. Best is trial 2 with value: 0.1395852454336066.
WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001709DFDF1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
[I 2024-09-28 23:39:07,765] Trial 4 finished with value: 0.24515216365613435 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'RNN', 'num_layers': 1, 'num_neurons': 32, 'dropout_rate': 0.1}. Best is trial 2 with value: 0.1395852454336066.
WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001709CCD8280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.
[I 2024-09-28 23:39:25,427] Trial 5 finished with value: 0.48123515500520403 and parameters: {'learning_rate': 0.0001, 'batch_size': 16, 'network_type': 'LSTM', 'num_layers': 3, 'num_neurons': 64, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:39:29,714] Trial 6 finished with value: 0.31717179464039047 and parameters: {'learning_rate': 0.01, 'batch_size': 16, 'network_type': 'RNN', 'num_layers': 1, 'num_neurons': 16, 'dropout_rate': 0.1}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:39:42,637] Trial 7 finished with value: 0.43601525221372905 and parameters: {'learning_rate': 0.0001, 'batch_size': 32, 'network_type': 'RNN', 'num_layers': 3, 'num_neurons': 64, 'dropout_rate': 0.3}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:39:49,779] Trial 8 finished with value: 0.4569083160601164 and parameters: {'learning_rate': 0.01, 'batch_size': 16, 'network_type': 'RNN', 'num_layers': 3, 'num_neurons': 32, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:40:00,068] Trial 9 finished with value: 0.45256543550993256 and parameters: {'learning_rate': 0.01, 'batch_size': 32, 'network_type': 'LSTM', 'num_layers': 3, 'num_neurons': 32, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:40:08,762] Trial 10 finished with value: 0.32531732147618336 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'GRU', 'num_layers': 2, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:40:14,125] Trial 11 finished with value: 0.29251551768654266 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'GRU', 'num_layers': 1, 'num_neurons': 32, 'dropout_rate': 0.1}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:40:19,579] Trial 12 finished with value: 0.3634506754122282 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'GRU', 'num_layers': 1, 'num_neurons': 16, 'dropout_rate': 0.1}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:40:24,551] Trial 13 finished with value: 0.3222434221066926 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'GRU', 'num_layers': 1, 'num_neurons': 32, 'dropout_rate': 0.1}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:40:29,448] Trial 14 finished with value: 0.18969369075172826 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'LSTM', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:40:35,067] Trial 15 finished with value: 0.2762703721397801 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'LSTM', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:40:44,557] Trial 16 finished with value: 0.2794354540674309 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'LSTM', 'num_layers': 2, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:40:49,803] Trial 17 finished with value: 0.3164794797897339 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'LSTM', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:40:54,799] Trial 18 finished with value: 0.2302565086766293 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'LSTM', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:41:02,482] Trial 19 finished with value: 0.2502355338648745 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'GRU', 'num_layers': 2, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:41:08,010] Trial 20 finished with value: 0.4329422640298542 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'GRU', 'num_layers': 1, 'num_neurons': 16, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:41:14,448] Trial 21 finished with value: 0.2570247689297325 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'LSTM', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:41:19,741] Trial 22 finished with value: 0.1980151077571668 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'LSTM', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:41:24,854] Trial 23 finished with value: 0.2037655819340756 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'LSTM', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:41:29,876] Trial 24 finished with value: 0.3432047569375289 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'LSTM', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:41:36,933] Trial 25 finished with value: 0.44413615919414307 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'LSTM', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:41:41,991] Trial 26 finished with value: 0.37256505378923915 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'LSTM', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:41:47,834] Trial 27 finished with value: 0.27435031830637074 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'GRU', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:41:58,576] Trial 28 finished with value: 0.5369911874720925 and parameters: {'learning_rate': 0.01, 'batch_size': 16, 'network_type': 'LSTM', 'num_layers': 3, 'num_neurons': 16, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:42:06,821] Trial 29 finished with value: 0.4806705375972546 and parameters: {'learning_rate': 0.01, 'batch_size': 32, 'network_type': 'GRU', 'num_layers': 2, 'num_neurons': 128, 'dropout_rate': 0.3}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:42:12,526] Trial 30 finished with value: 0.29637772299114024 and parameters: {'learning_rate': 0.01, 'batch_size': 16, 'network_type': 'LSTM', 'num_layers': 1, 'num_neurons': 64, 'dropout_rate': 0.3}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:42:17,540] Trial 31 finished with value: 0.2333708011727584 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'LSTM', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:42:23,588] Trial 32 finished with value: 0.3229246090838783 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'LSTM', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:42:29,598] Trial 33 finished with value: 0.4609331057197168 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'LSTM', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:42:34,549] Trial 34 finished with value: 0.2567922581120541 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'LSTM', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:42:39,484] Trial 35 finished with value: 0.20868966594495272 and parameters: {'learning_rate': 0.01, 'batch_size': 32, 'network_type': 'LSTM', 'num_layers': 1, 'num_neurons': 64, 'dropout_rate': 0.3}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:42:43,242] Trial 36 finished with value: 0.36755732877630937 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'RNN', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.2}. Best is trial 2 with value: 0.1395852454336066.
[I 2024-09-28 23:42:48,066] Trial 37 finished with value: 0.13752683980841385 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'GRU', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.1}. Best is trial 37 with value: 0.13752683980841385.
[I 2024-09-28 23:42:59,908] Trial 38 finished with value: 0.43140124411331976 and parameters: {'learning_rate': 0.01, 'batch_size': 16, 'network_type': 'GRU', 'num_layers': 3, 'num_neurons': 64, 'dropout_rate': 0.1}. Best is trial 37 with value: 0.13752683980841385.
[I 2024-09-28 23:43:07,739] Trial 39 finished with value: 0.3730011941508242 and parameters: {'learning_rate': 0.01, 'batch_size': 32, 'network_type': 'GRU', 'num_layers': 2, 'num_neurons': 128, 'dropout_rate': 0.1}. Best is trial 37 with value: 0.13752683980841385.
[I 2024-09-28 23:43:12,659] Trial 40 finished with value: 0.33315796114269053 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'GRU', 'num_layers': 1, 'num_neurons': 16, 'dropout_rate': 0.1}. Best is trial 37 with value: 0.13752683980841385.
[I 2024-09-28 23:43:16,636] Trial 41 finished with value: 0.3542636082297877 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'RNN', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.1}. Best is trial 37 with value: 0.13752683980841385.
[I 2024-09-28 23:43:21,526] Trial 42 finished with value: 0.1415800434915643 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'GRU', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.1}. Best is trial 37 with value: 0.13752683980841385.
[I 2024-09-28 23:43:26,682] Trial 43 finished with value: 0.15668849081742137 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'GRU', 'num_layers': 1, 'num_neurons': 128, 'dropout_rate': 0.1}. Best is trial 37 with value: 0.13752683980841385.
[I 2024-09-28 23:43:38,390] Trial 44 finished with value: 0.41090200589832504 and parameters: {'learning_rate': 0.01, 'batch_size': 64, 'network_type': 'GRU', 'num_layers': 3, 'num_neurons': 32, 'dropout_rate': 0.1}. Best is trial 37 with value: 0.13752683980841385.
